{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pytorch_lightning as pl\n",
    "import flash \n",
    "from flash.image import ImageClassifier\n",
    "from flash.core.data.data_module import DataModule\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "arr = np.random.randn(10, 5, 3)\n",
    "arr = arr.transpose(2, 0, 1)\n",
    "arr.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 10, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    #imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1080, 64, 64, 3) (1080,)\n",
      "(120, 64, 64, 3) (120,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_tensor_arr = [to_tensor(img) for img in train_x]\n",
    "# stack will arrange the tensors one over the other with dim=0 being the new dimension that  \n",
    "# stores the number of tensors stacked. This new dimension can be placed at any index\n",
    "img_tensor_arr = torch.stack(img_tensor_arr)\n",
    "img_tensor_arr.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1080, 3, 64, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# CONSTANTS\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class NpArrayImageDataset(Dataset):\n",
    "    def __init__(self, img_arr, label_arr, transform):\n",
    "        self.img_arr = img_arr\n",
    "        self.label_arr = label_arr\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:            \n",
    "            tfmd_img = self.transform(self.img_arr[index])\n",
    "            # ToTensor transformation causes the RGB channel dimension to shift from index 2 to 0\n",
    "            # we interchange the dimensions at index 0 and 2 to move channel dim back to index 2\n",
    "        #return (tfmd_img.transpose(0,2), self.label_arr[index])\n",
    "        return (tfmd_img, self.label_arr[index])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=NUM_FOLDS, X=train_x, y=train_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_imgs_mean_stddev(imgs, axis=None):    \n",
    "    \"\"\"Get the mean and standard deviation for images in a dataset / mini-batch\n",
    "    Args:\n",
    "        imgs ([2d or 3d numpy array]): images in collection (with no to_tensor transformation applied)\n",
    "        axis ([tuple of ints], optional): Axis along which mean and std dev is to be calculated.\n",
    "        Defaults to None.\n",
    "    Returns:\n",
    "        [tuple]: tuple of tensors with mean and std.dev. of the imgs\n",
    "    \"\"\"\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    img_tensor_arr = [to_tensor(img) for img in train_x]\n",
    "    # stack will arrange the tensors one over the other with dim=0 being the new dimension that  \n",
    "    # stores the number of tensors stacked. This new dimension can be placed at any index\n",
    "    img_tensor_arr = torch.stack(img_tensor_arr)\n",
    "    if axis is not None:\n",
    "        return torch.mean(img_tensor_arr, axis=axis), torch.std(img_tensor_arr, axis=axis)\n",
    "    else:            \n",
    "        return torch.mean(img_tensor_arr, axis=(0, 2, 3)), torch.std(img_tensor_arr, axis=(0,2,3))\n",
    "    \n",
    "train_img_mean, train_img_std = get_imgs_mean_stddev(train_x)        \n",
    "print(train_img_mean, train_img_std)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.7630, 0.7105, 0.6634]) tensor([0.1538, 0.1998, 0.2221])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize(train_img_mean, train_img_std)])\n",
    "\n",
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_ds(fold, kfolds, X, y):                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]\n",
    "    ds_train_signs = NpArrayImageDataset(train_X, train_y, transform=img_transforms)\n",
    "    ds_val_signs = NpArrayImageDataset(val_X, val_y, transform=img_transforms)    \n",
    "    return ds_train_signs, ds_val_signs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from flash.image.classification.transforms import default_transforms\n",
    "from flash.core.data.transforms import ApplyToKeys\n",
    "from flash.core.data.data_source import DefaultDataKeys\n",
    "\n",
    "ds_train, ds_val = get_fold_ds(0, k_folds, train_x, train_y)\n",
    "\n",
    "train_transform = {        \n",
    "        \"post_tensor_transform\": ApplyToKeys(DefaultDataKeys.INPUT, transforms.Normalize(torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])))\n",
    "    }\n",
    "\n",
    "data_module = DataModule.from_datasets(\n",
    "    train_dataset=ds_train,\n",
    "    val_dataset=ds_val, \n",
    "    train_transform=train_transform, \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "img0 = data_module.train_dataset[0]['input']\n",
    "img0.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from flash.image import ImageClassifier\n",
    "model = ImageClassifier(backbone=\"resnet18\", num_classes=6 )\n",
    "\n",
    "trainer = flash.Trainer(max_epochs=1, gpus=torch.cuda.device_count())\n",
    "trainer.finetune(model, datamodule=data_module, strategy=\"freeze\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | train_metrics | ModuleDict     | 0     \n",
      "1 | val_metrics   | ModuleDict     | 0     \n",
      "2 | adapter       | DefaultAdapter | 11.2 M\n",
      "-------------------------------------------------\n",
      "12.7 K    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.718    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b234e311064642278c444b28e14ca9c0"
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09531d8f4c4c485cb91d2929dbe2f527"
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)"
  },
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}