{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torchmetrics\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "from optuna_integration import PyTorchLightningPruningCallback\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 3, 64, 64) (1080,)\n",
      "(120, 3, 64, 64) (120,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\"\n",
    "\n",
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_CLASSES = 6\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 30\n",
    "    TRAIN_IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "    TRAIN_IMG_STD = [0.229, 0.224, 0.225]\n",
    "    UNFREEZE_EPOCH_NO = 2\n",
    "    PRECISION = \"16-mixed\"\n",
    "    PATIENCE = 6\n",
    "    LOG_EVERY_N_STEPS = 10\n",
    "    MODEL_TO_USE = Models.RESNET50\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.005,\n",
    "        \"warmup_prop\": 0.05\n",
    "    }\n",
    "\n",
    "class SchedulerConfig:\n",
    "    SCHEDULER_PATIENCE = 3\n",
    "    SCHEDULER = \"ReduceLROnPlateau\"\n",
    "    T_0 = 10 # for CosineAnnealingWarmRestarts\n",
    "    MIN_LR = 5e-7 # for CosineAnnealingWarmRestarts\n",
    "    MAX_LR = 1e-2\n",
    "    STEPS_PER_EPOCH = 13 # for OneCycleLR\n",
    "\n",
    "class WandbConfig:\n",
    "    WANDB_KEY = \"\"\n",
    "    WANDB_RUN_NAME = \"pl_cnn_signs_resnet50\"\n",
    "    WANDB_PROJECT = \"pl_cnn_signs\"\n",
    "    USE_WANDB = True    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(cfg):\n",
    "    # dir is an inbuilt python function that returns the list of attributes and methods of any object\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config_to_dict(Config)\n",
    "schd_config_dict = config_to_dict(SchedulerConfig)\n",
    "merged_config_dict = {**config_dict, **schd_config_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=Config.NUM_FOLDS, X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpArrayImageDataset(Dataset):\n",
    "    def __init__(self, img_arr, label_arr, transform, target_transform, \n",
    "                transform_type=TransformationType.TORCHVISION):\n",
    "        self.img_arr = img_arr\n",
    "        self.label_arr = label_arr\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transform_type = transform_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tfmd_img = self.img_arr[index]\n",
    "        tfmd_img = tfmd_img.transpose(1,2,0)\n",
    "        #print(type(tfmd_img), tfmd_img.shape)\n",
    "        tfmd_label = self.label_arr[index]\n",
    "        if self.transform:\n",
    "            if self.transform_type == TransformationType.TORCHVISION:                        \n",
    "                tfmd_img = self.transform(tfmd_img)\n",
    "            elif self.transform_type == TransformationType.ALB:\n",
    "                augmented = self.transform(image=tfmd_img)\n",
    "                tfmd_img = augmented[\"image\"]                   \n",
    "        if self.target_transform:               \n",
    "            tfmd_label = self.target_transform(tfmd_label)              \n",
    "        return tfmd_img, tfmd_label            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize(Config.TRAIN_IMG_MEAN, Config.TRAIN_IMG_STD)])\n",
    "\n",
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, kfolds, X, y):\n",
    "    fold += 1                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]\n",
    "    ds_train = NpArrayImageDataset(train_X, train_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    ds_val = NpArrayImageDataset(val_X, val_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]            \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(Image.fromarray(np.uint8(img)).convert('RGB'))        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)\n",
    "#len(ds_val)\n",
    "show_batch(ds_val, 3, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config_dict = config_dict                \n",
    "        self.num_classes = config_dict[\"NUM_CLASSES\"]\n",
    "        self.lr = config_dict[\"MODEL_PARAMS\"][\"lr\"]\n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(\n",
    "            config_dict[\"MODEL_TO_USE\"],\n",
    "            config_dict[\"MODEL_PARAMS\"][\"drop_out\"], \n",
    "            self.num_classes\n",
    "        ) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):        \n",
    "        params = self.parameters()\n",
    "        print(f\"len(params) = {len(list(params))}\")\n",
    "        return dl_utils.get_optimizer(lr=self.lr, params=self.parameters(), config_dict=self.config_dict)    \n",
    "    \n",
    "    # def configure_optimizers(self):\n",
    "    #     model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=Config.WEIGHT_DECAY)\n",
    "    #     print(f\"param groups count = {len(model_optimizer.param_groups)}\")\n",
    "    #     lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\", patience=SchedulerConfig.SCHEDULER_PATIENCE)        \n",
    "    #     return {\n",
    "    #         \"optimizer\": model_optimizer, \n",
    "    #         \"lr_scheduler\": {\n",
    "    #             \"scheduler\": lr_scheduler,\n",
    "    #             \"monitor\": \"val_loss\",\n",
    "    #             \"frequency\": 1\n",
    "    #         }\n",
    "    #     }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"cur_lr\", current_lr, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n",
    "        return {\"loss\": val_loss, \"val_acc\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaPruning(PyTorchLightningPruningCallback, pl.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hparam_tuning(model_params, trial):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)    \n",
    "    early_stopping = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    multiplicative = lambda epoch: 1.5\n",
    "    backbone_finetuning_cb = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=False)\n",
    "    signs_model = ImageClassificationLitModel(config_dict=merged_config_dict)  \n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        # For results reproducibility \n",
    "        deterministic=True,\n",
    "        strategy=\"auto\",\n",
    "        log_every_n_steps=Config.LOG_EVERY_N_STEPS,\n",
    "        max_epochs=Config.NUM_EPOCHS,        \n",
    "        precision=Config.PRECISION,   \n",
    "        enable_model_summary=True,\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[backbone_finetuning_cb, early_stopping]\n",
    "    )      \n",
    "    trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n",
    "    loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    del trainer, signs_model, early_stopping, backbone_finetuning_cb, dl_train, dl_val\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"lr\": trial.suggest_float(\"lr\", low=1e-6, high=1e-3, log=True),\n",
    "#         \"drop_out\": trial.suggest_float(\"drop_out\", low=0.2, high=0.7)\n",
    "#     }    \n",
    "#     loss = run_hparam_tuning(params, trial)\n",
    "#     return loss\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"SignsImageClassificationTuning\")    \n",
    "# study.optimize(objective, n_trials=10)\n",
    "# print(f\"Best trial number = {study.best_trial.number}\")\n",
    "# print(\"Best trial params:\")\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "        self.val_epoch_num = 0\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.val_epoch_num += 1\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].cpu().detach().item()\n",
    "        current_lr = trainer.callback_metrics[\"cur_lr\"].cpu().detach().item()\n",
    "        print(f\"epoch = {self.val_epoch_num} => metric {self.metric_to_monitor} = {metric_value}, \" \\\n",
    "              f\"val_loss={val_loss}, lr={current_lr}\")\n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "def get_wandb_logger(fold, config_dict=None):\n",
    "    logger = None\n",
    "    if WandbConfig.USE_WANDB:        \n",
    "        wandb.login(key=WandbConfig.WANDB_KEY)        \n",
    "        logger = WandbLogger(\n",
    "            name=WandbConfig.WANDB_RUN_NAME + f\"_fold{fold}\", \n",
    "            project=WandbConfig.WANDB_PROJECT,\n",
    "            config=config_dict,\n",
    "            group=Config.MODEL_TO_USE\n",
    "        )\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "    try:\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")        \n",
    "        logger = get_wandb_logger(fold, merged_config_dict)\n",
    "        print(\"Instantiated wandb logger\")    \n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"        \n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "        signs_model = ImageClassificationLitModel(config_dict=merged_config_dict)    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            devices=\"auto\",\n",
    "            accelerator=\"gpu\",\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            strategy=\"auto\",\n",
    "            log_every_n_steps=Config.LOG_EVERY_N_STEPS,\n",
    "            max_epochs=Config.NUM_EPOCHS,        \n",
    "            precision=Config.PRECISION,   \n",
    "            enable_model_summary=True,\n",
    "            enable_progress_bar=True,                        \n",
    "            logger=logger,            \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        tuner = Tuner(trainer)\n",
    "        \n",
    "        if find_lr:\n",
    "            lr_finder = tuner.lr_find(model=signs_model, train_dataloaders=dl_train)\n",
    "            # Results can be found in\n",
    "            print(lr_finder.results)\n",
    "            # Results can be plotted to identify the optimal learning rate\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "            # Pick the suggested learning rate\n",
    "            new_lr = lr_finder.suggestion()\n",
    "            print(f\"new_lr = {new_lr}\")\n",
    "\n",
    "        trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        loss = loss_chkpt_callback.best_model_score.cpu().detach().item()\n",
    "        acc = acc_chkpt_callback.best_metric\n",
    "        print(f\"Loss for {fold_str} = {loss}, accuracy = {acc}\")\n",
    "        del trainer, tuner, signs_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback                 \n",
    "        return loss, acc\n",
    "    except KeyboardInterrupt as e:\n",
    "        wandb.finish(exit_code=-1, quiet=True)\n",
    "        print(\"Marked the wandb run as failed\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbkanupam\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/bk_anupam/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated wandb logger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:52: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e41f686c12e477198a76c0f2d8477a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113344522244814, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240802_134832-97h7rwdl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bkanupam/pl_cnn_signs/runs/97h7rwdl' target=\"_blank\">pl_cnn_signs_resnet50_fold0</a></strong> to <a href='https://wandb.ai/bkanupam/pl_cnn_signs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bkanupam/pl_cnn_signs' target=\"_blank\">https://wandb.ai/bkanupam/pl_cnn_signs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bkanupam/pl_cnn_signs/runs/97h7rwdl' target=\"_blank\">https://wandb.ai/bkanupam/pl_cnn_signs/runs/97h7rwdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | backbone   | Sequential | 23.5 M | train\n",
      "1 | classifier | Sequential | 12.3 K | train\n",
      "--------------------------------------------------\n",
      "65.4 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(params) = 161\n",
      "param groups count = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f2e07dfd6e43b996eb82de14da3828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 => metric val_acc = 0.1640625, val_loss=1.8243789672851562, lr=0.004999999888241291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6c0cf18ffd4fbb84e6c4f4ee7e064a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2437d93a43c94bf0be40e1cd299c53e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.104\n",
      "Epoch 0, global step 14: 'val_loss' reached 1.10354 (best 1.10354), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=0_val_loss=1.1035.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2 => metric val_acc = 0.5694444179534912, val_loss=1.1035401821136475, lr=0.004999999422580004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0559c3069ce4a04934de6e2c817672b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.422 >= min_delta = 0.0. New best score: 0.681\n",
      "Epoch 1, global step 28: 'val_loss' reached 0.68118 (best 0.68118), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=1_val_loss=0.6812.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3 => metric val_acc = 0.7685185074806213, val_loss=0.6811829209327698, lr=0.004999999422580004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/finetuning.py:238: The provided params to be frozen already exist within another group of this optimizer. Those parameters will be skipped.\n",
      "HINT: Did you init your optimizer in `configure_optimizer` as such:\n",
      " <class 'torch.optim.adam.Adam'>(filter(lambda p: p.requires_grad, self.parameters()), ...) \n",
      "Current lr: 0.005, Backbone lr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b64265874346178fd11d24cd68051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 42: 'val_loss' was not in top 1\n",
      "Current lr: 0.005, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4 => metric val_acc = 0.26851850748062134, val_loss=2.3487935066223145, lr=0.004999999422580004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f551b1a2f54e3d8f33951118ba76c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.404 >= min_delta = 0.0. New best score: 0.277\n",
      "Epoch 3, global step 56: 'val_loss' reached 0.27672 (best 0.27672), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=3_val_loss=0.2767.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5 => metric val_acc = 0.8888888955116272, val_loss=0.2767183184623718, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229ae805ebde464d8637aefc0e92913e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.155 >= min_delta = 0.0. New best score: 0.122\n",
      "Epoch 4, global step 70: 'val_loss' reached 0.12176 (best 0.12176), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=4_val_loss=0.1218.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6 => metric val_acc = 0.9722222089767456, val_loss=0.12175639718770981, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b22c45ede74684b06b1671aa2c6fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.115\n",
      "Epoch 5, global step 84: 'val_loss' reached 0.11474 (best 0.11474), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=5_val_loss=0.1147.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7 => metric val_acc = 0.9722222089767456, val_loss=0.11473800241947174, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d12baa2eb594f37ba73abc22fb32501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.113\n",
      "Epoch 6, global step 98: 'val_loss' reached 0.11330 (best 0.11330), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=6_val_loss=0.1133.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8 => metric val_acc = 0.9768518805503845, val_loss=0.11329903453588486, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a695b885d11f4bfd901ee46e7a81eef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.112\n",
      "Epoch 7, global step 112: 'val_loss' reached 0.11170 (best 0.11170), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=7_val_loss=0.1117.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9 => metric val_acc = 0.9768518805503845, val_loss=0.11169567704200745, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd91d3f92bcc443c8ec3d2a61b11ccf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.107\n",
      "Epoch 8, global step 126: 'val_loss' reached 0.10743 (best 0.10743), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.1074.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10 => metric val_acc = 0.9722222089767456, val_loss=0.1074310764670372, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df58e1f3a5af4b50bb92684534079999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 140: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11 => metric val_acc = 0.9722222089767456, val_loss=0.11418222635984421, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d5b575e2c34d5ab9064a2f1d4886b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 154: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12 => metric val_acc = 0.9675925970077515, val_loss=0.11910085380077362, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4330c97c3b194955b39a2389bdc602ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 168: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13 => metric val_acc = 0.9675925970077515, val_loss=0.10768663138151169, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6521bf1e024c658737fa046966159d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.096\n",
      "Epoch 12, global step 182: 'val_loss' reached 0.09604 (best 0.09604), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=12_val_loss=0.0960.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14 => metric val_acc = 0.9722222089767456, val_loss=0.09604063630104065, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c76cec176a94bb2811177e6834f1689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.086\n",
      "Epoch 13, global step 196: 'val_loss' reached 0.08621 (best 0.08621), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=13_val_loss=0.0862.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15 => metric val_acc = 0.9768518805503845, val_loss=0.08620984107255936, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a8f703f1a54782a7faebed004d6fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.082\n",
      "Epoch 14, global step 210: 'val_loss' reached 0.08237 (best 0.08237), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=14_val_loss=0.0824.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16 => metric val_acc = 0.9814814925193787, val_loss=0.08237394690513611, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baa515cc5704d5eb8f7cafacb3edeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 224: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 17 => metric val_acc = 0.9768518805503845, val_loss=0.08452874422073364, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d05fcf0159547ddb81833d5312a91e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.082\n",
      "Epoch 16, global step 238: 'val_loss' reached 0.08227 (best 0.08227), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=16_val_loss=0.0823-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18 => metric val_acc = 0.9768518805503845, val_loss=0.08226781338453293, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f39adcc4aeb407997e780b951b3b419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 252: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 19 => metric val_acc = 0.9768518805503845, val_loss=0.0873212218284607, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a92c4397eef47689848cffcb7f53ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 266: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20 => metric val_acc = 0.9768518805503845, val_loss=0.09113413095474243, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bae899feca43e59f193037d4b672a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 280: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 21 => metric val_acc = 0.9768518805503845, val_loss=0.08655253052711487, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5685fa82b30424682c7d0ac3f672654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 294: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 22 => metric val_acc = 0.9768518805503845, val_loss=0.09025446325540543, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b257bdc3156045d4bab77bc0b23c630c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 308: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 23 => metric val_acc = 0.9768518805503845, val_loss=0.0894472524523735, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353feffd89994b8f80baa7b662824497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 6 records. Best score: 0.082. Signaling Trainer to stop.\n",
      "Epoch 22, global step 322: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 24 => metric val_acc = 0.9814814925193787, val_loss=0.09083932638168335, lr=7.500000356230885e-05\n",
      "Loss for fold0 = 0.08226781338453293, accuracy = 0.9814814925193787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec838ca243d4f41a28f806d59fa11da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>cur_lr_epoch</td><td>â–ˆâ–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>cur_lr_step</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–</td></tr><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_acc_epoch</td><td>â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_acc_step</td><td>â–â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train_loss_epoch</td><td>â–ˆâ–„â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss_step</td><td>â–ˆâ–…â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–‚â–â–‚â–â–â–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚â–„â–‚â–„â–…â–‚â–…â–‚â–…â–‚â–‚â–†â–‚â–†â–ƒâ–‡â–ƒâ–ƒâ–‡â–ƒâ–ˆâ–ƒâ–ˆâ–ˆ</td></tr><tr><td>val_acc_epoch</td><td>â–„â–†â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_acc_step</td><td>â–ƒâ–…â–†â–†â–‚â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss_epoch</td><td>â–„â–ƒâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val_loss_step</td><td>â–„â–„â–ƒâ–ƒâ–ˆâ–ˆâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cur_lr_epoch</td><td>8e-05</td></tr><tr><td>cur_lr_step</td><td>8e-05</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>train_acc_epoch</td><td>1.0</td></tr><tr><td>train_acc_step</td><td>1.0</td></tr><tr><td>train_loss_epoch</td><td>0.00053</td></tr><tr><td>train_loss_step</td><td>8e-05</td></tr><tr><td>trainer/global_step</td><td>321</td></tr><tr><td>val_acc_epoch</td><td>0.98148</td></tr><tr><td>val_acc_step</td><td>1.0</td></tr><tr><td>val_loss_epoch</td><td>0.09084</td></tr><tr><td>val_loss_step</td><td>0.02947</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pl_cnn_signs_resnet50_fold0</strong> at: <a href='https://wandb.ai/bkanupam/pl_cnn_signs/runs/97h7rwdl' target=\"_blank\">https://wandb.ai/bkanupam/pl_cnn_signs/runs/97h7rwdl</a><br/> View project at: <a href='https://wandb.ai/bkanupam/pl_cnn_signs' target=\"_blank\">https://wandb.ai/bkanupam/pl_cnn_signs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_134832-97h7rwdl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, k_folds, train_x, train_y)\n",
    "    loss, acc = run_training(fold, dl_train, dl_val, find_lr=False)\n",
    "    fold_loss.append(loss)\n",
    "    fold_acc.append(acc)\n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss across folds\n",
      "[0.08226781338453293]\n",
      "Accuracy across folds\n",
      "[0.9814814925193787]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Loss across folds\")\n",
    "print(fold_loss)\n",
    "print(\"Accuracy across folds\")\n",
    "print(fold_acc)\n",
    "\n",
    "if len(fold_loss) > 1:\n",
    "    mean_loss = statistics.mean(fold_loss)\n",
    "    mean_acc = statistics.mean(fold_acc)\n",
    "    std_loss = statistics.stdev(fold_loss)\n",
    "    std_acc = statistics.stddev(fold_acc)\n",
    "    print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "    print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "# model = ImageClassificationLitModel.load_from_checkpoint(\n",
    "#     checkpoint_path=\"model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt\",     \n",
    "#     num_classes=Config.NUM_CLASSES\n",
    "#     )\n",
    "# model.to(\"cuda\")\n",
    "# model.eval()\n",
    "\n",
    "# incorrect = 0\n",
    "# total = 0\n",
    "# predicted_labels_incorrect = []\n",
    "# labels_incorrect = []\n",
    "# with torch.no_grad():\n",
    "#     counter=0\n",
    "#     for imgs, labels in tqdm.tqdm(dl_val):                \n",
    "#         predicted_cuda_labels = torch.argmax(model(imgs.to(\"cuda\")), dim=1)\n",
    "#         predicted_labels = predicted_cuda_labels.cpu().detach()\n",
    "#         total += labels.shape[0]\n",
    "#         correct_pred = predicted_labels == labels\n",
    "#         incorrect_pred = ~correct_pred\n",
    "#         num_incorrect_pred = incorrect_pred.sum()\n",
    "#         incorrect += int(num_incorrect_pred)\n",
    "#         if num_incorrect_pred > 0:\n",
    "#             predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "#             labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "# print(f'Total no. of images in validation set: {total}')\n",
    "# print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "# accuracy = ((total-incorrect) / total) * 100        \n",
    "# print(f\"Accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
