{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.13 (you have 1.4.12). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import wandb\n",
    "import torchmetrics\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "from optuna_integration import PyTorchLightningPruningCallback\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 3, 64, 64) (1080,)\n",
      "(120, 3, 64, 64) (120,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\"\n",
    "\n",
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_CLASSES = 6\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 30\n",
    "    TRAIN_IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "    TRAIN_IMG_STD = [0.229, 0.224, 0.225]\n",
    "    UNFREEZE_EPOCH_NO = 2\n",
    "    PRECISION = \"16-mixed\"\n",
    "    PATIENCE = 6\n",
    "    LOG_EVERY_N_STEPS = 10\n",
    "    MODEL_TO_USE = Models.RESNET50\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.005,\n",
    "        \"warmup_prop\": 0.05\n",
    "    }\n",
    "    INPUT_IMAGE_SIZE = (64, 64)\n",
    "    TRANSFORMATION_TYPE = TransformationType.ALB\n",
    "\n",
    "class SchedulerConfig:\n",
    "    SCHEDULER_PATIENCE = 3\n",
    "    SCHEDULER = \"ReduceLROnPlateau\"\n",
    "    T_0 = 10 # for CosineAnnealingWarmRestarts\n",
    "    MIN_LR = 5e-7 # for CosineAnnealingWarmRestarts\n",
    "    MAX_LR = 1e-2\n",
    "    STEPS_PER_EPOCH = 13 # for OneCycleLR\n",
    "\n",
    "class WandbConfig:\n",
    "    WANDB_KEY = \"\"\n",
    "    WANDB_RUN_NAME = \"pl_cnn_signs_resnet50\"\n",
    "    WANDB_PROJECT = \"pl_cnn_signs\"\n",
    "    USE_WANDB = False    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(cfg):\n",
    "    # dir is an inbuilt python function that returns the list of attributes and methods of any object\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config_to_dict(Config)\n",
    "schd_config_dict = config_to_dict(SchedulerConfig)\n",
    "merged_config_dict = {**config_dict, **schd_config_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=Config.NUM_FOLDS, X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpArrayImageDataset(Dataset):\n",
    "    def __init__(self, img_arr, label_arr, transform, target_transform, \n",
    "                transform_type=TransformationType.TORCHVISION):\n",
    "        self.img_arr = img_arr\n",
    "        self.label_arr = label_arr\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transform_type = transform_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tfmd_img = self.img_arr[index]\n",
    "        tfmd_img = tfmd_img.transpose(1,2,0)\n",
    "        #print(type(tfmd_img), tfmd_img.shape)\n",
    "        tfmd_label = self.label_arr[index]\n",
    "        if self.transform:\n",
    "            if self.transform_type == TransformationType.TORCHVISION:                        \n",
    "                tfmd_img = self.transform(tfmd_img)\n",
    "            elif self.transform_type == TransformationType.ALB:\n",
    "                augmented = self.transform(image=tfmd_img)\n",
    "                tfmd_img = augmented[\"image\"]                   \n",
    "        if self.target_transform:               \n",
    "            tfmd_label = self.target_transform(tfmd_label)              \n",
    "        return tfmd_img, tfmd_label            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_transforms = transforms.Compose([v2.ToTensor(), \n",
    "#                                      v2.Normalize(Config.TRAIN_IMG_MEAN, Config.TRAIN_IMG_STD)])\n",
    "\n",
    "# img_transforms = v2.Compose([#v2.RandomResizedCrop([Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]]),\n",
    "#                             v2.RandomHorizontalFlip(p=0.5),\n",
    "#                             v2.RandomVerticalFlip(p=0.5), \n",
    "#                             v2.RandomRotation(45),                                                                         \n",
    "#                             v2.Normalize(Config.TRAIN_IMG_MEAN, Config.TRAIN_IMG_STD),\n",
    "#                             v2.ToTensor()])\n",
    "\n",
    "train_transform = alb.Compose([\n",
    "        alb.RandomResizedCrop(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]),        \n",
    "        alb.HorizontalFlip(p=0.5),\n",
    "        alb.VerticalFlip(p=0.5),\n",
    "        alb.RandomRotate90(p=0.5),\n",
    "        alb.Flip(p=0.5),\n",
    "        alb.Downscale(p=0.25),\n",
    "        alb.ShiftScaleRotate(shift_limit=0.1, \n",
    "                           scale_limit=0.15, \n",
    "                           rotate_limit=60, \n",
    "                           p=0.5),\n",
    "        alb.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, \n",
    "                sat_shift_limit=0.2, \n",
    "                val_shift_limit=0.2, \n",
    "                p=0.5\n",
    "            ),\n",
    "        alb.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1,0.1), \n",
    "                contrast_limit=(-0.1, 0.1), \n",
    "                p=0.5\n",
    "            ),\n",
    "        alb.Normalize(mean=Config.TRAIN_IMG_MEAN, std=Config.TRAIN_IMG_STD),\n",
    "        ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = alb.Compose([\n",
    "        alb.CenterCrop(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]),\n",
    "        alb.Normalize(mean=Config.TRAIN_IMG_MEAN, std=Config.TRAIN_IMG_STD),\n",
    "        ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, kfolds, X, y):\n",
    "    fold += 1                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]\n",
    "    ds_train = NpArrayImageDataset(train_X, train_y, transform=train_transform, \n",
    "                                   target_transform=torch.as_tensor, transform_type=Config.TRANSFORMATION_TYPE)\n",
    "    ds_val = NpArrayImageDataset(val_X, val_y, transform=val_transform, target_transform=torch.as_tensor,\n",
    "                                 transform_type=Config.TRANSFORMATION_TYPE)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]            \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(Image.fromarray(np.uint8(img)).convert('RGB'))        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)\n",
    "show_batch(ds_val, 3, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, config_dict):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config_dict = config_dict                \n",
    "        self.num_classes = config_dict[\"NUM_CLASSES\"]\n",
    "        self.lr = config_dict[\"MODEL_PARAMS\"][\"lr\"]\n",
    "        self.cutmix = v2.CutMix(num_classes=config_dict[\"NUM_CLASSES\"])\n",
    "        self.mixup = v2.MixUp(num_classes=config_dict[\"NUM_CLASSES\"])        \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(\n",
    "            config_dict[\"MODEL_TO_USE\"],\n",
    "            config_dict[\"MODEL_PARAMS\"][\"drop_out\"], \n",
    "            self.num_classes\n",
    "        ) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):        \n",
    "        params = self.parameters()\n",
    "        print(f\"len(params) = {len(list(params))}\")\n",
    "        return dl_utils.get_optimizer(lr=self.lr, params=self.parameters(), config_dict=self.config_dict)    \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.cutmix_or_mixup = v2.RandomChoice([self.cutmix, self.mixup])\n",
    "        X, y = batch\n",
    "        #print(f\"Before CutMix/MixUp: {X.shape = }, {y.shape = }\")\n",
    "        X, y = self.cutmix_or_mixup(X, y)\n",
    "        #print(f\"After CutMix/MixUp: {X.shape = }, {y.shape = }\")        \n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes)\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"cur_lr\", current_lr, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n",
    "        return {\"loss\": val_loss, \"val_acc\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaPruning(PyTorchLightningPruningCallback, pl.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hparam_tuning(model_params, trial):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)    \n",
    "    early_stopping = OptunaPruning(trial, monitor=\"val_loss\")\n",
    "    multiplicative = lambda epoch: 1.5\n",
    "    backbone_finetuning_cb = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=False)\n",
    "    signs_model = ImageClassificationLitModel(config_dict=merged_config_dict)  \n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"gpu\",\n",
    "        # For results reproducibility \n",
    "        deterministic=True,\n",
    "        strategy=\"auto\",\n",
    "        log_every_n_steps=Config.LOG_EVERY_N_STEPS,\n",
    "        max_epochs=Config.NUM_EPOCHS,        \n",
    "        precision=Config.PRECISION,   \n",
    "        enable_model_summary=True,\n",
    "        enable_progress_bar=True,\n",
    "        callbacks=[backbone_finetuning_cb, early_stopping]\n",
    "    )      \n",
    "    trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n",
    "    loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    del trainer, signs_model, early_stopping, backbone_finetuning_cb, dl_train, dl_val\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"lr\": trial.suggest_float(\"lr\", low=1e-6, high=1e-3, log=True),\n",
    "#         \"drop_out\": trial.suggest_float(\"drop_out\", low=0.2, high=0.7)\n",
    "#     }    \n",
    "#     loss = run_hparam_tuning(params, trial)\n",
    "#     return loss\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"SignsImageClassificationTuning\")    \n",
    "# study.optimize(objective, n_trials=10)\n",
    "# print(f\"Best trial number = {study.best_trial.number}\")\n",
    "# print(\"Best trial params:\")\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "    try:\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        logger = None\n",
    "        if WandbConfig.USE_WANDB:                \n",
    "            logger = dl_utils.get_wandb_logger(fold, merged_config_dict)\n",
    "        print(\"Instantiated wandb logger\")    \n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"        \n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "        signs_model = ImageClassificationLitModel(config_dict=merged_config_dict)    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = dl_utils.MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            devices=\"auto\",\n",
    "            accelerator=\"gpu\",\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            strategy=\"auto\",\n",
    "            log_every_n_steps=Config.LOG_EVERY_N_STEPS,\n",
    "            max_epochs=Config.NUM_EPOCHS,        \n",
    "            precision=Config.PRECISION,   \n",
    "            enable_model_summary=True,\n",
    "            enable_progress_bar=True,                        \n",
    "            logger=logger,            \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        tuner = Tuner(trainer)\n",
    "        \n",
    "        if find_lr:\n",
    "            lr_finder = tuner.lr_find(model=signs_model, train_dataloaders=dl_train)\n",
    "            # Results can be found in\n",
    "            print(lr_finder.results)\n",
    "            # Results can be plotted to identify the optimal learning rate\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "            # Pick the suggested learning rate\n",
    "            new_lr = lr_finder.suggestion()\n",
    "            print(f\"new_lr = {new_lr}\")\n",
    "\n",
    "        trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        loss = loss_chkpt_callback.best_model_score.cpu().detach().item()\n",
    "        acc = acc_chkpt_callback.best_metric\n",
    "        print(f\"Loss for {fold_str} = {loss}, accuracy = {acc}\")\n",
    "        del trainer, tuner, signs_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback                 \n",
    "        return loss, acc\n",
    "    except KeyboardInterrupt as e:\n",
    "        wandb.finish(exit_code=-1, quiet=True)\n",
    "        print(\"Marked the wandb run as failed\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n",
      "Instantiated wandb logger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:52: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | cutmix     | CutMix     | 0      | train\n",
      "1 | mixup      | MixUp      | 0      | train\n",
      "2 | backbone   | Sequential | 23.5 M | train\n",
      "3 | classifier | Sequential | 12.3 K | train\n",
      "--------------------------------------------------\n",
      "65.4 K    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.081    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(params) = 161\n",
      "param groups count = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8de353482947ab956b1057c0400b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1 => metric val_acc = 0.1640625, val_loss=1.8243789672851562, lr=0.004999999888241291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7622738fe744a5d81864b8dc84d1b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b3aa55f504b4f93b6d87130353ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.150\n",
      "INFO: Epoch 0, global step 14: 'val_loss' reached 1.14977 (best 1.14977), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=0_val_loss=1.1498.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 0, global step 14: 'val_loss' reached 1.14977 (best 1.14977), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=0_val_loss=1.1498.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 2 => metric val_acc = 0.5833333134651184, val_loss=1.1497708559036255, lr=0.004999999422580004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02b51f8809d40fbad428e9fcead78c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.241 >= min_delta = 0.0. New best score: 0.909\n",
      "INFO: Epoch 1, global step 28: 'val_loss' reached 0.90857 (best 0.90857), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=1_val_loss=0.9086.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 1, global step 28: 'val_loss' reached 0.90857 (best 0.90857), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=1_val_loss=0.9086.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 3 => metric val_acc = 0.6574074029922485, val_loss=0.9085701704025269, lr=0.004999999422580004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/finetuning.py:238: The provided params to be frozen already exist within another group of this optimizer. Those parameters will be skipped.\n",
      "HINT: Did you init your optimizer in `configure_optimizer` as such:\n",
      " <class 'torch.optim.adam.Adam'>(filter(lambda p: p.requires_grad, self.parameters()), ...) \n",
      "Current lr: 0.005, Backbone lr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e006852c60149ff93bee25a29031b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 2, global step 42: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 2, global step 42: 'val_loss' was not in top 1\n",
      "Current lr: 0.005, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 4 => metric val_acc = 0.23148147761821747, val_loss=3.267014980316162, lr=0.004999999422580004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd877494fcd41dda77f6e306ef302ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.881\n",
      "INFO: Epoch 3, global step 56: 'val_loss' reached 0.88137 (best 0.88137), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=3_val_loss=0.8814.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 3, global step 56: 'val_loss' reached 0.88137 (best 0.88137), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=3_val_loss=0.8814.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 5 => metric val_acc = 0.6435185074806213, val_loss=0.8813731670379639, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f894db636b6d4c399de70d300d0d2c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.237 >= min_delta = 0.0. New best score: 0.645\n",
      "INFO: Epoch 4, global step 70: 'val_loss' reached 0.64453 (best 0.64453), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=4_val_loss=0.6445.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 4, global step 70: 'val_loss' reached 0.64453 (best 0.64453), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=4_val_loss=0.6445.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6 => metric val_acc = 0.7453703880310059, val_loss=0.6445304751396179, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623af32f4a6d4adaa3a5d19333e90f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.074 >= min_delta = 0.0. New best score: 0.571\n",
      "INFO: Epoch 5, global step 84: 'val_loss' reached 0.57100 (best 0.57100), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=5_val_loss=0.5710.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 5, global step 84: 'val_loss' reached 0.57100 (best 0.57100), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=5_val_loss=0.5710.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 7 => metric val_acc = 0.7361111044883728, val_loss=0.5709969401359558, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2b0910f0624ffd8bf20f64a07b70a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.201 >= min_delta = 0.0. New best score: 0.370\n",
      "INFO: Epoch 6, global step 98: 'val_loss' reached 0.37027 (best 0.37027), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=6_val_loss=0.3703.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 6, global step 98: 'val_loss' reached 0.37027 (best 0.37027), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=6_val_loss=0.3703.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 8 => metric val_acc = 0.8703703880310059, val_loss=0.3702690601348877, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bd36181f9b4fbc92b2c09287079e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.093 >= min_delta = 0.0. New best score: 0.277\n",
      "INFO: Epoch 7, global step 112: 'val_loss' reached 0.27686 (best 0.27686), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=7_val_loss=0.2769.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 7, global step 112: 'val_loss' reached 0.27686 (best 0.27686), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=7_val_loss=0.2769.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 9 => metric val_acc = 0.9027777910232544, val_loss=0.2768634855747223, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb5c26bc5e946cba8fbd6e7021fc357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.055 >= min_delta = 0.0. New best score: 0.222\n",
      "INFO: Epoch 8, global step 126: 'val_loss' reached 0.22211 (best 0.22211), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.2221.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 8, global step 126: 'val_loss' reached 0.22211 (best 0.22211), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.2221.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 10 => metric val_acc = 0.9305555820465088, val_loss=0.22211343050003052, lr=0.000750000006519258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e87be1e695c4c2aa40ebcc58f76a9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 9, global step 140: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 9, global step 140: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 11 => metric val_acc = 0.9166666865348816, val_loss=0.23783019185066223, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239fcfd876c4400fa56be755a7d4dfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 10, global step 154: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 10, global step 154: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 12 => metric val_acc = 0.9305555820465088, val_loss=0.2278435379266739, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451118e6d86a4192942981089e92f259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 11, global step 168: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 11, global step 168: 'val_loss' was not in top 1\n",
      "Current lr: 0.00075, Backbone lr: 0.00075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 13 => metric val_acc = 0.8935185074806213, val_loss=0.2976182997226715, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c263b168f0054f9cb149fd2b2b2bdf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 12, global step 182: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 12, global step 182: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 14 => metric val_acc = 0.8611111044883728, val_loss=0.36914464831352234, lr=0.000750000006519258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925f044f8e8b4c70ac2ea042f0df83a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 13, global step 196: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 13, global step 196: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 15 => metric val_acc = 0.9259259104728699, val_loss=0.22568638622760773, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79faf2971d814485911488bbda65245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.054 >= min_delta = 0.0. New best score: 0.168\n",
      "INFO: Epoch 14, global step 210: 'val_loss' reached 0.16765 (best 0.16765), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=14_val_loss=0.1676.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 14, global step 210: 'val_loss' reached 0.16765 (best 0.16765), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=14_val_loss=0.1676.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 16 => metric val_acc = 0.9629629850387573, val_loss=0.16764695942401886, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad88e2a55343456fa4bdbb04baa60be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.0. New best score: 0.153\n",
      "INFO: Epoch 15, global step 224: 'val_loss' reached 0.15299 (best 0.15299), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=15_val_loss=0.1530.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 15, global step 224: 'val_loss' reached 0.15299 (best 0.15299), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=15_val_loss=0.1530.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 17 => metric val_acc = 0.9675925970077515, val_loss=0.1529911905527115, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226f1808ce2b46898afd08e53fd19d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.146\n",
      "INFO: Epoch 16, global step 238: 'val_loss' reached 0.14631 (best 0.14631), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=16_val_loss=0.1463.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 16, global step 238: 'val_loss' reached 0.14631 (best 0.14631), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=16_val_loss=0.1463.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 18 => metric val_acc = 0.9675925970077515, val_loss=0.14630864560604095, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f6926cc2fc4150a409eb9faf7e7cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.138\n",
      "INFO: Epoch 17, global step 252: 'val_loss' reached 0.13821 (best 0.13821), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=17_val_loss=0.1382.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 17, global step 252: 'val_loss' reached 0.13821 (best 0.13821), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=17_val_loss=0.1382.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 19 => metric val_acc = 0.9629629850387573, val_loss=0.1382102370262146, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b024a2dde541bda4f40b5b1062cd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 18, global step 266: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 18, global step 266: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20 => metric val_acc = 0.9629629850387573, val_loss=0.14297567307949066, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021ad9e5bd90497ab346aad3693b3be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 19, global step 280: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 19, global step 280: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 21 => metric val_acc = 0.9675925970077515, val_loss=0.14524562656879425, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389939a30e5f42bc9c235869fe7bb62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 20, global step 294: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 20, global step 294: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 22 => metric val_acc = 0.9722222089767456, val_loss=0.14123548567295074, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e087244fffe0466c8a2c5e768aa6c11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.137\n",
      "INFO: Epoch 21, global step 308: 'val_loss' reached 0.13713 (best 0.13713), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=21_val_loss=0.1371.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 21, global step 308: 'val_loss' reached 0.13713 (best 0.13713), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=21_val_loss=0.1371.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 23 => metric val_acc = 0.9629629850387573, val_loss=0.137130469083786, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53c09615adf43ad8bcdab536126f713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.132\n",
      "INFO: Epoch 22, global step 322: 'val_loss' reached 0.13231 (best 0.13231), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=22_val_loss=0.1323.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 22, global step 322: 'val_loss' reached 0.13231 (best 0.13231), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=22_val_loss=0.1323.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 24 => metric val_acc = 0.9675925970077515, val_loss=0.13230565190315247, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68636ccd65614d9fb789ca28f6147ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 23, global step 336: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 23, global step 336: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 25 => metric val_acc = 0.9629629850387573, val_loss=0.13783852756023407, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e12eaab63b34baa9e9451675b8cbbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 24, global step 350: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 24, global step 350: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 26 => metric val_acc = 0.9583333134651184, val_loss=0.137502059340477, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6e6773f8a460f9b1d5bc2217bae2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.125\n",
      "INFO: Epoch 25, global step 364: 'val_loss' reached 0.12537 (best 0.12537), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=25_val_loss=0.1254.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 25, global step 364: 'val_loss' reached 0.12537 (best 0.12537), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=25_val_loss=0.1254.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 27 => metric val_acc = 0.9537037014961243, val_loss=0.1253703087568283, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4df30f4e034c0687efb154c780b5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 26, global step 378: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 26, global step 378: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 28 => metric val_acc = 0.9583333134651184, val_loss=0.12808692455291748, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d64f4533e346aba01f87656b96aa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 27, global step 392: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 27, global step 392: 'val_loss' was not in top 1\n",
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 29 => metric val_acc = 0.9490740895271301, val_loss=0.12707674503326416, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa83a0502614a92a655f6bd814eea13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.119\n",
      "INFO: Epoch 28, global step 406: 'val_loss' reached 0.11853 (best 0.11853), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=28_val_loss=0.1185.ckpt' as top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 28, global step 406: 'val_loss' reached 0.11853 (best 0.11853), saving model to '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=28_val_loss=0.1185.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 30 => metric val_acc = 0.9629629850387573, val_loss=0.11853355914354324, lr=7.500000356230885e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 7.5e-05, Backbone lr: 7.5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70db7f8cf8244ea86fb41c33120850a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Epoch 29, global step 420: 'val_loss' was not in top 1\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Epoch 29, global step 420: 'val_loss' was not in top 1\n",
      "INFO: `Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 31 => metric val_acc = 0.9537037014961243, val_loss=0.12030928581953049, lr=7.500000356230885e-05\n",
      "Loss for fold0 = 0.11853355914354324, accuracy = 0.9722222089767456\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, k_folds, train_x, train_y)\n",
    "    loss, acc = run_training(fold, dl_train, dl_val, find_lr=False)\n",
    "    fold_loss.append(loss)\n",
    "    fold_acc.append(acc)\n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss across folds\n",
      "[0.11853355914354324]\n",
      "Accuracy across folds\n",
      "[0.9722222089767456]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Loss across folds\")\n",
    "print(fold_loss)\n",
    "print(\"Accuracy across folds\")\n",
    "print(fold_acc)\n",
    "\n",
    "if len(fold_loss) > 1:\n",
    "    mean_loss = statistics.mean(fold_loss)\n",
    "    mean_acc = statistics.mean(fold_acc)\n",
    "    std_loss = statistics.stdev(fold_loss)\n",
    "    std_acc = statistics.stddev(fold_acc)\n",
    "    print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "    print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = alb.Compose([\n",
    "        alb.CenterCrop(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1]),\n",
    "        alb.Normalize(mean=Config.TRAIN_IMG_MEAN, std=Config.TRAIN_IMG_STD),\n",
    "        ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = NpArrayImageDataset(test_x, test_y, transform=test_transform, target_transform=torch.as_tensor, \n",
    "                              transform_type=Config.TRANSFORMATION_TYPE)\n",
    "dl_test = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of images in test set: 120\n",
      "Incorrectly classified images in test set: 3\n",
      "Accuracy: 97.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "model = ImageClassificationLitModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"model/fold0_best_model_epoch=28_val_loss=0.1185.ckpt\",     \n",
    "    num_classes=Config.NUM_CLASSES\n",
    "    )\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "incorrect = 0\n",
    "total = 0\n",
    "predicted_labels_incorrect = []\n",
    "labels_incorrect = []\n",
    "with torch.no_grad():\n",
    "    counter=0\n",
    "    for imgs, labels in tqdm.tqdm(dl_test):                \n",
    "        predicted_cuda_labels = torch.argmax(model(imgs.to(\"cuda\")), dim=1)\n",
    "        predicted_labels = predicted_cuda_labels.detach().cpu()\n",
    "        total += labels.shape[0]\n",
    "        correct_pred = predicted_labels == labels\n",
    "        incorrect_pred = ~correct_pred\n",
    "        num_incorrect_pred = incorrect_pred.sum()\n",
    "        incorrect += int(num_incorrect_pred)\n",
    "        if num_incorrect_pred > 0:\n",
    "            predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "            labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "print(f'Total no. of images in test set: {total}')\n",
    "print(f'Incorrectly classified images in test set: {incorrect}')\n",
    "accuracy = ((total-incorrect) / total) * 100        \n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
