{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pytorch_lightning as pl\n",
    "import flash \n",
    "from flash.image import ImageClassifier\n",
    "from flash.core.data.data_module import DataModule\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "arr = np.random.randn(10, 5, 3)\n",
    "arr = arr.transpose(2, 0, 1)\n",
    "arr.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 10, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1080, 3, 64, 64) (1080,)\n",
      "(120, 3, 64, 64) (120,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "# CONSTANTS\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "#img = Image.fromarray(np.uint8(test_x[0])).convert('RGB')\n",
    "#img"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "from flash.core.data.data_source import DataSource, DefaultDataKeys\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from typing import Any, Dict, Iterable , Mapping, Sequence, Callable   \n",
    "\n",
    "class SignsDataSource(DataSource):    \n",
    "    def load_data(self, h5_file_path: str) -> Sequence[Mapping[str, Any]]:\n",
    "        f = h5py.File(h5_file_path, \"r\")\n",
    "        ds_keys = [key for key in f.keys()]\n",
    "        img_arr = np.array(f[ds_keys[1]])    \n",
    "        label_arr = np.array(f[ds_keys[2]])        \n",
    "        return [\n",
    "            {\n",
    "                DefaultDataKeys.INPUT: img,\n",
    "                DefaultDataKeys.TARGET: label\n",
    "            } \n",
    "            for img, label in list(zip(img_arr, label_arr))]\n",
    "\n",
    "    def load_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        sample[DefaultDataKeys.INPUT] = Image.fromarray(np.uint8(sample[DefaultDataKeys.INPUT])).convert('RGB')\n",
    "        return sample\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "from typing import Optional\n",
    "from flash.core.data.process import Preprocess\n",
    "from flash.core.data.data_source import DefaultDataKeys, DefaultDataSources\n",
    "\n",
    "class SignsPreprocess(Preprocess):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_transform: Optional[Dict[str, Callable]] = None,\n",
    "        val_transform: Optional[Dict[str, Callable]] = None,\n",
    "        test_transform: Optional[Dict[str, Callable]] = None,\n",
    "        predict_transform: Optional[Dict[str, Callable]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            train_transform=train_transform,\n",
    "            val_transform=val_transform,\n",
    "            test_transform=test_transform,\n",
    "            predict_transform=predict_transform,\n",
    "            data_sources={\n",
    "                DefaultDataSources.FILES: SignsDataSource(),\n",
    "            },\n",
    "            default_data_source=DefaultDataSources.FILES,\n",
    "        )\n",
    "\n",
    "    def default_transforms(self) -> Dict[str, Callable]:\n",
    "        return {\n",
    "            \"to_tensor_transform\": transforms.ToTensor(),\n",
    "            \"post_tensor_transform\": transforms.Normalize(\n",
    "                torch.tensor([0.485, 0.456, 0.406]), \n",
    "                torch.tensor([0.229, 0.224, 0.225])\n",
    "                ),\n",
    "            \"collate\": torch.utils.data._utils.collate.default_collate\n",
    "        }        \n",
    "\n",
    "    def get_state_dict(self) -> Dict[str, Any]:\n",
    "        return {**self.transforms}\n",
    "\n",
    "    @classmethod\n",
    "    def load_state_dict(cls, state_dict: Dict[str, Any], strict: bool = False):\n",
    "        return cls(**state_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=NUM_FOLDS, X=train_x, y=train_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "def split_data(fold, kfolds, X, y):\n",
    "    train_X = X[kfolds != fold+1]        \n",
    "    train_y = y[kfolds != fold+1]    \n",
    "    val_X = X[kfolds == fold+1]\n",
    "    val_y = y[kfolds == fold+1]\n",
    "    return train_X, train_y, val_X, val_y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "from flash.image.classification.transforms import default_transforms\n",
    "from flash.core.data.transforms import ApplyToKeys\n",
    "from flash.core.data.data_source import DefaultDataKeys\n",
    "from flash.image import ImageClassificationData\n",
    "\n",
    "train_X, train_y, val_X, val_y = split_data(0, k_folds, train_x, train_y)\n",
    "\n",
    "signs_default_transform = {\n",
    "    \"to_tensor_transform\": nn.Sequential(\n",
    "            ApplyToKeys(DefaultDataKeys.INPUT, transforms.ToTensor()),\n",
    "            ApplyToKeys(DefaultDataKeys.TARGET, torch.as_tensor),\n",
    "        ),\n",
    "        \"post_tensor_transform\": ApplyToKeys(\n",
    "            DefaultDataKeys.INPUT,\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ),\n",
    "    \"collate\": torch.utils.data._utils.collate.default_collate\n",
    "}\n",
    "\n",
    "data_module = ImageClassificationData.from_numpy(\n",
    "    train_data = train_X,\n",
    "    train_targets= train_y,\n",
    "    val_data = val_X,\n",
    "    val_targets = val_y,\n",
    "    #test_data = test_x,\n",
    "    #test_targets = test_y,\n",
    "    train_transform = signs_default_transform,\n",
    "    val_transform = signs_default_transform,\n",
    "    #test_transform = signs_default_transform,\n",
    "    #predict_transform = signs_default_transform,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    num_workers = NUM_WORKERS\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "item = data_module.val_dataset[10]\n",
    "print(item[DefaultDataKeys.TARGET])\n",
    "type(item[DefaultDataKeys.INPUT])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "test_y[10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "source": [
    "from flash.image import ImageClassifier\n",
    "model = ImageClassifier(backbone=\"resnet18\", num_classes=6 )\n",
    "\n",
    "trainer = flash.Trainer(max_epochs=10, gpus=torch.cuda.device_count())\n",
    "trainer.finetune(model, datamodule=data_module, strategy=\"freeze\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | train_metrics | ModuleDict     | 0     \n",
      "1 | val_metrics   | ModuleDict     | 0     \n",
      "2 | adapter       | DefaultAdapter | 11.2 M\n",
      "-------------------------------------------------\n",
      "12.7 K    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.718    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5781319099064604b19cbaa83bc96031"
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "931b064b02ed4916a0f7f67dc9f1734f"
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7410d0106bb0413b8c8f974b0a80293a"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd1d82371593433ebaf185d372d3ef17"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87df62e3af6e47a9afc439c813365dc9"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ff8f0147d704ca2b1c5afa26e58d031"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "554c7ff5e933495aa695e46bffe9867a"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fcb46a5ad464ea085705fb5c5b2431e"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0a6978d13b9445085900e0ffa046c2f"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4fe62ffdab844b684c990bcefe84dca"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5102f96762ba487f9bc2ff11b4f95a32"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6ab6d01bfaa44e7b5c47a988dd9b191"
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "model.predict([test_x[110]], data_source=DefaultDataSources.NUMPY)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "metadata": {},
     "execution_count": 179
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)"
  },
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}