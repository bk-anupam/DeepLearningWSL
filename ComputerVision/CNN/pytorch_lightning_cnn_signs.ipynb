{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 3, 64, 64) (1080,)\n",
      "(120, 3, 64, 64) (120,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "class Config:\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_CLASSES = 6\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    TRAIN_IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "    TRAIN_IMG_STD = [0.229, 0.224, 0.225]\n",
    "    UNFREEZE_EPOCH_NO = 2\n",
    "    PRECISION = 16\n",
    "    PATIENCE = 5\n",
    "\n",
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=Config.NUM_FOLDS, X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpArrayImageDataset(Dataset):\n",
    "    def __init__(self, img_arr, label_arr, transform, target_transform, \n",
    "                transform_type=TransformationType.TORCHVISION):\n",
    "        self.img_arr = img_arr\n",
    "        self.label_arr = label_arr\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transform_type = transform_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tfmd_img = self.img_arr[index]\n",
    "        tfmd_img = tfmd_img.transpose(1,2,0)\n",
    "        #print(type(tfmd_img), tfmd_img.shape)\n",
    "        tfmd_label = self.label_arr[index]\n",
    "        if self.transform:\n",
    "            if self.transform_type == TransformationType.TORCHVISION:                        \n",
    "                tfmd_img = self.transform(tfmd_img)\n",
    "            elif self.transform_type == TransformationType.ALB:\n",
    "                augmented = self.transform(image=tfmd_img)\n",
    "                tfmd_img = augmented[\"image\"]                   \n",
    "        if self.target_transform:               \n",
    "            tfmd_label = self.target_transform(tfmd_label)              \n",
    "        return tfmd_img, tfmd_label            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize(Config.TRAIN_IMG_MEAN, Config.TRAIN_IMG_STD)])\n",
    "\n",
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, kfolds, X, y):\n",
    "    fold += 1                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]\n",
    "    ds_train = NpArrayImageDataset(train_X, train_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    ds_val = NpArrayImageDataset(val_X, val_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]            \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(Image.fromarray(np.uint8(img)).convert('RGB'))        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)\n",
    "#len(ds_val)\n",
    "show_batch(ds_val, 3, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, hparams, model_to_use):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.num_classes = num_classes        \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(model_to_use, hparams[\"drop_out\"], num_classes) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"val_acc\": val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# model hyperparameters\n",
    "model_params = {    \n",
    "    \"drop_out\": 0.25,\n",
    "    \"lr\": 0.001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "def run_hparam_tuning(model_params, trial):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)    \n",
    "    early_stopping = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    multiplicative = lambda epoch: 1.5\n",
    "    backbone_finetuning_cb = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=False)\n",
    "    signs_model = ImageClassificationLitModel(\n",
    "        num_classes=Config.NUM_CLASSES, \n",
    "        hparams=model_params,        \n",
    "        model_to_use=Models.RESNET50\n",
    "        )  \n",
    "    trainer = pl.Trainer(\n",
    "        #checkpoint_callback=False,        \n",
    "        devices=1,\n",
    "        accelerator=\"gpu\",\n",
    "        # For results reproducibility \n",
    "        deterministic=True,\n",
    "        auto_select_gpus=True,\n",
    "        progress_bar_refresh_rate=20,\n",
    "        max_epochs=Config.NUM_EPOCHS,        \n",
    "        precision=Config.PRECISION,   \n",
    "        weights_summary=None,                     \n",
    "        callbacks=[backbone_finetuning_cb, early_stopping]\n",
    "    )      \n",
    "    trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n",
    "    loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    del trainer, signs_model, early_stopping, backbone_finetuning_cb, dl_train, dl_val\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-31 09:35:35,005] A new study created in memory with name: SignsImageClassificationTuning\n",
      "/tmp/ipykernel_13640/3460718823.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3),\n",
      "/tmp/ipykernel_13640/3460718823.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"drop_out\": trial.suggest_uniform(\"drop_out\", 0.2, 0.7)\n",
      "[W 2024-07-31 09:35:36,118] Trial 0 failed with parameters: {'lr': 3.608910482145584e-06, 'drop_out': 0.3406537073429493} because of the following error: TypeError(\"Trainer.__init__() got an unexpected keyword argument 'gpus'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13640/3460718823.py\", line 8, in objective\n",
      "    loss = run_hparam_tuning(params, trial)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13640/1857990460.py\", line 13, in run_hparam_tuning\n",
      "    trainer = pl.Trainer(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/bk_anupam/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py\", line 70, in insert_env_defaults\n",
      "    return fn(self, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Trainer.__init__() got an unexpected keyword argument 'gpus'\n",
      "[W 2024-07-31 09:35:36,119] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'gpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     11\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSignsImageClassificationTuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m---> 12\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial number = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mnumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial params:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      4\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_loguniform(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m),\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_out\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_uniform(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      7\u001b[0m     }    \n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_hparam_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mrun_hparam_tuning\u001b[0;34m(model_params, trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m backbone_finetuning_cb \u001b[38;5;241m=\u001b[39m BackboneFinetuning(Config\u001b[38;5;241m.\u001b[39mUNFREEZE_EPOCH_NO, multiplicative, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m signs_model \u001b[38;5;241m=\u001b[39m ImageClassificationLitModel(\n\u001b[1;32m      9\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mNUM_CLASSES, \n\u001b[1;32m     10\u001b[0m     hparams\u001b[38;5;241m=\u001b[39mmodel_params,        \n\u001b[1;32m     11\u001b[0m     model_to_use\u001b[38;5;241m=\u001b[39mModels\u001b[38;5;241m.\u001b[39mRESNET50\n\u001b[1;32m     12\u001b[0m     )  \n\u001b[0;32m---> 13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#checkpoint_callback=False,        \u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For results reproducibility \u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar_refresh_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRECISION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbackbone_finetuning_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[1;32m     25\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(signs_model, train_dataloaders\u001b[38;5;241m=\u001b[39mdl_train, val_dataloaders\u001b[38;5;241m=\u001b[39mdl_val)     \n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'gpus'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3),\n",
    "        \"drop_out\": trial.suggest_uniform(\"drop_out\", 0.2, 0.7)\n",
    "    }    \n",
    "    loss = run_hparam_tuning(params, trial)\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"SignsImageClassificationTuning\")    \n",
    "study.optimize(objective, n_trials=10)\n",
    "print(f\"Best trial number = {study.best_trial.number}\")\n",
    "print(\"Best trial params:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}\")\n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "\n",
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        tb_logger = None\n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"        \n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "        #print_table_metric_cb = PrintTableMetricsCallback()\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", version=fold_str)\n",
    "        else:\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")        \n",
    "        signs_model = ImageClassificationLitModel(\n",
    "            num_classes=Config.NUM_CLASSES, \n",
    "            hparams=model_params,        \n",
    "            model_to_use=Models.RESNET50\n",
    "            )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            auto_select_gpus=True,\n",
    "            progress_bar_refresh_rate=20,\n",
    "            max_epochs=Config.NUM_EPOCHS,\n",
    "            logger=tb_logger,\n",
    "            auto_lr_find=True,    \n",
    "            precision=Config.PRECISION,    \n",
    "            weights_summary=None,                    \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        if find_lr:\n",
    "            trainer.tune(model=signs_model, train_dataloaders=dl_train)\n",
    "            print(signs_model.lr)\n",
    "        trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        fold_loss.append(loss_chkpt_callback.best_model_score.cpu().detach().item())\n",
    "        fold_acc.append(acc_chkpt_callback.best_metric)\n",
    "        print(f\"Loss for {fold_str} = {fold_loss[fold]}, accuracy = {fold_acc[fold]}\")\n",
    "        del trainer, signs_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (14) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243087d79ce34e5da9dff42f238179d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0010964781961431851\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010964781961431851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648b957c09c04fa99b8d479c0c71e31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.1328125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38569b68a82e4c738e46e87117c40da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1876f76a7a404a9b8984ca70c5e155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.293\n",
      "Epoch 0, global step 13: val_loss reached 1.29323 (best 1.29323), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=0_val_loss=1.2932.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.46296295523643494\n",
      "metric val_acc = 0.46296295523643494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a571b6009a2462a8fb7c33e644012a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.344 >= min_delta = 0.0. New best score: 0.950\n",
      "Epoch 1, global step 27: val_loss reached 0.94973 (best 0.94973), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=1_val_loss=0.9497.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.7870370149612427\n",
      "metric val_acc = 0.7870370149612427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/finetuning.py:215: UserWarning: The provided params to be freezed already exist within another group of this optimizer. Those parameters will be skipped.\n",
      "HINT: Did you init your optimizer in `configure_optimizer` as such:\n",
      " <class 'torch.optim.adam.Adam'>(filter(lambda p: p.requires_grad, self.parameters()), ...) \n",
      "  rank_zero_warn(\n",
      "Current lr: 0.001096478196, Backbone lr: 0.00010964782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50337b84c0394e369ef79cc7e27eeaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.772 >= min_delta = 0.0. New best score: 0.177\n",
      "Epoch 2, global step 41: val_loss reached 0.17749 (best 0.17749), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=2_val_loss=0.1775.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000164471729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daea1666b72743e6afa99f00dce30964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.079 >= min_delta = 0.0. New best score: 0.098\n",
      "Epoch 3, global step 55: val_loss reached 0.09819 (best 0.09819), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=3_val_loss=0.0982.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000246707594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e4a9eb81664e0596e5bf3c78389e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 69: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000370061391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9305555820465088\n",
      "metric val_acc = 0.9305555820465088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55b2593886e4021a8feaf3c7805c834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000555092087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8888888955116272\n",
      "metric val_acc = 0.8888888955116272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69350c61692549e68f3679408ee54777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.00083263813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9351851940155029\n",
      "metric val_acc = 0.9351851940155029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35358bfda734f4496f50c7ed1b7eb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.001096478196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8472222089767456\n",
      "metric val_acc = 0.8472222089767456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1fa1c86fe34d88a8b8ddab34a08b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.085\n",
      "Epoch 8, global step 125: val_loss reached 0.08531 (best 0.08531), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.001096478196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec699d30fd0e454eac5c3d2d7d736ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 139: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n",
      "metric val_acc = 0.9537037014961243\n",
      "Loss for fold0 = 0.08530975133180618, accuracy = 0.9768518805503845\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, k_folds, train_x, train_y)\n",
    "    run_training(fold, dl_train, dl_val)\n",
    "    break\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss across folds\n",
      "[0.08530975133180618]\n",
      "Accuracy across folds\n",
      "[0.9768518805503845]\n",
      "mean loss across folds = 0.08530975133180618\n",
      "mean accuracy across folds = 0.9768518805503845\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(\"Loss across folds\")\n",
    "print(fold_loss)\n",
    "print(\"Accuracy across folds\")\n",
    "print(fold_acc)\n",
    "\n",
    "mean_loss = statistics.mean(fold_loss)\n",
    "mean_acc = statistics.mean(fold_acc)\n",
    "std_loss = statistics.stdev(fold_loss)\n",
    "std_acc = statistics.stddev(fold_acc)\n",
    "print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "\n",
    "# model = ImageClassificationLitModel.load_from_checkpoint(\n",
    "#     checkpoint_path=\"model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt\",     \n",
    "#     num_classes=Config.NUM_CLASSES\n",
    "#     )\n",
    "# model.to(\"cuda\")\n",
    "# model.eval()\n",
    "\n",
    "# incorrect = 0\n",
    "# total = 0\n",
    "# predicted_labels_incorrect = []\n",
    "# labels_incorrect = []\n",
    "# with torch.no_grad():\n",
    "#     counter=0\n",
    "#     for imgs, labels in tqdm.tqdm(dl_val):                \n",
    "#         predicted_cuda_labels = torch.argmax(model(imgs.to(\"cuda\")), dim=1)\n",
    "#         predicted_labels = predicted_cuda_labels.cpu().detach()\n",
    "#         total += labels.shape[0]\n",
    "#         correct_pred = predicted_labels == labels\n",
    "#         incorrect_pred = ~correct_pred\n",
    "#         num_incorrect_pred = incorrect_pred.sum()\n",
    "#         incorrect += int(num_incorrect_pred)\n",
    "#         if num_incorrect_pred > 0:\n",
    "#             predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "#             labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "# print(f'Total no. of images in validation set: {total}')\n",
    "# print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "# accuracy = ((total-incorrect) / total) * 100        \n",
    "# print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
