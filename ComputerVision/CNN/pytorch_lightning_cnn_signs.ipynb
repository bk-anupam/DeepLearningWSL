{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 3, 64, 64) (1080,)\n",
      "(120, 3, 64, 64) (120,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (signs)\n",
    "def get_imgs_labels(h5_file_path):\n",
    "    f = h5py.File(h5_file_path, \"r\")\n",
    "    ds_keys = [key for key in f.keys()]\n",
    "    imgs = np.array(f[ds_keys[1]])    \n",
    "    labels = np.array(f[ds_keys[2]])\n",
    "    list_classes = np.array(f[ds_keys[0]])\n",
    "    imgs = np.transpose(imgs, (0, 3, 1, 2))\n",
    "    return imgs, labels, list_classes\n",
    "\n",
    "train_x, train_y, train_classes = get_imgs_labels(\"./datasets/train_signs.h5\")\n",
    "test_x, test_y, test_classes = get_imgs_labels(\"./datasets/test_signs.h5\")\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "#timm.list_models(filter=\"efficient*\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "class Config:\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_CLASSES = 6\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "    NUM_EPOCHS = 10\n",
    "    TRAIN_IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "    TRAIN_IMG_STD = [0.229, 0.224, 0.225]\n",
    "    UNFREEZE_EPOCH_NO = 2\n",
    "    PRECISION = 16\n",
    "    PATIENCE = 5\n",
    "\n",
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a training and label data in form of numpy arrays, return a fold_index array whose elements\n",
    "# represent the fold index. The length of this fold_index array is same as length of input dataset\n",
    "# and the items for which fold_index array value == cv iteration count are to be used for validation \n",
    "# in the corresponding cross validation iteration with rest of the items ( for which fold_index \n",
    "# array value != cv iteration count ) being used for training (typical ration being 80:20)\n",
    "def get_skf_index(num_folds, X, y):\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state = 42)\n",
    "    train_fold_index = np.zeros(len(y))\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=X, y=y)):\n",
    "        train_fold_index[val_index] = [fold + 1] * len(val_index)\n",
    "    return train_fold_index\n",
    "\n",
    "k_folds = get_skf_index(num_folds=Config.NUM_FOLDS, X=train_x, y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpArrayImageDataset(Dataset):\n",
    "    def __init__(self, img_arr, label_arr, transform, target_transform, \n",
    "                transform_type=TransformationType.TORCHVISION):\n",
    "        self.img_arr = img_arr\n",
    "        self.label_arr = label_arr\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transform_type = transform_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_arr)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tfmd_img = self.img_arr[index]\n",
    "        tfmd_img = tfmd_img.transpose(1,2,0)\n",
    "        #print(type(tfmd_img), tfmd_img.shape)\n",
    "        tfmd_label = self.label_arr[index]\n",
    "        if self.transform:\n",
    "            if self.transform_type == TransformationType.TORCHVISION:                        \n",
    "                tfmd_img = self.transform(tfmd_img)\n",
    "            elif self.transform_type == TransformationType.ALB:\n",
    "                augmented = self.transform(image=tfmd_img)\n",
    "                tfmd_img = augmented[\"image\"]                   \n",
    "        if self.target_transform:               \n",
    "            tfmd_label = self.target_transform(tfmd_label)              \n",
    "        return tfmd_img, tfmd_label            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                     transforms.Normalize(Config.TRAIN_IMG_MEAN, Config.TRAIN_IMG_STD)])\n",
    "\n",
    "# Get the train and validation data loaders for a specific fold. \n",
    "# X: numpy array of input features\n",
    "# y: numpy array of target labels\n",
    "# fold: fold index for which to create data loaders                                     \n",
    "# kfolds: Array that marks each of the data items as belonging to a specific fold\n",
    "def get_fold_dls(fold, kfolds, X, y):\n",
    "    fold += 1                         \n",
    "    train_X = X[kfolds != fold]        \n",
    "    train_y = y[kfolds != fold]    \n",
    "    val_X = X[kfolds == fold]\n",
    "    val_y = y[kfolds == fold]\n",
    "    ds_train = NpArrayImageDataset(train_X, train_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    ds_val = NpArrayImageDataset(val_X, val_y, transform=img_transforms, target_transform=torch.as_tensor)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)\n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]            \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(Image.fromarray(np.uint8(img)).convert('RGB'))        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)\n",
    "#len(ds_val)\n",
    "show_batch(ds_val, 3, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class ImageClassificationModel(nn.Module):\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def __init__(self, num_classes, drop_out=0.25, model_to_use=\"resnext\"):\n",
    "        super().__init__()                \n",
    "        self.num_classes = num_classes        \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(model_to_use, drop_out, num_classes)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, hparams, model_to_use):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.num_classes = num_classes        \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(model_to_use, hparams[\"drop_out\"], num_classes) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_acc = accuracy(y_pred, y)\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", val_acc, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"val_acc\": val_acc}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        X, y = batch\n",
    "        # this calls forward\n",
    "        return self(X)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        return super().on_train_end()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "# model hyperparameters\n",
    "model_params = {    \n",
    "    \"drop_out\": 0.25,\n",
    "    \"lr\": 0.001\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "def run_hparam_tuning(model_params, trial):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, k_folds, train_x, train_y)    \n",
    "    early_stopping = PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")\n",
    "    backbone_finetuning_cb = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=False)\n",
    "    signs_model = ImageClassificationLitModel(\n",
    "        num_classes=Config.NUM_CLASSES, \n",
    "        hparams=model_params,        \n",
    "        model_to_use=Models.RESNET50\n",
    "        )  \n",
    "    trainer = pl.Trainer(\n",
    "        checkpoint_callback=False,        \n",
    "        gpus=1,\n",
    "        # For results reproducibility \n",
    "        deterministic=True,\n",
    "        auto_select_gpus=True,\n",
    "        progress_bar_refresh_rate=20,\n",
    "        max_epochs=Config.NUM_EPOCHS,        \n",
    "        precision=Config.PRECISION,   \n",
    "        weights_summary=None,                     \n",
    "        callbacks=[backbone_finetuning_cb, early_stopping]\n",
    "    )      \n",
    "    trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)     \n",
    "    loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    del trainer, signs_model, early_stopping, backbone_finetuning_cb, dl_train, dl_val\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"lr\": trial.suggest_loguniform(\"lr\", 1e-6, 1e-3),\n",
    "#         \"drop_out\": trial.suggest_uniform(\"drop_out\", 0.2, 0.7)\n",
    "#     }    \n",
    "#     loss = run_hparam_tuning(params, trial)\n",
    "#     return loss\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\", study_name=\"SignsImageClassificationTuning\")    \n",
    "# study.optimize(objective, n_trials=10)\n",
    "# print(f\"Best trial number = {study.best_trial.number}\")\n",
    "# print(\"Best trial params:\")\n",
    "# print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}\")\n",
    "        self.metrics.append(trainer.callback_metrics[self.metric_to_monitor].item())\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        tb_logger = None\n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"\n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "        #print_table_metric_cb = PrintTableMetricsCallback()\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", version=fold_str)\n",
    "        else:\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")        \n",
    "        signs_model = ImageClassificationLitModel(\n",
    "            num_classes=Config.NUM_CLASSES, \n",
    "            hparams=model_params,        \n",
    "            model_to_use=Models.RESNET50\n",
    "            )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            auto_select_gpus=True,\n",
    "            progress_bar_refresh_rate=20,\n",
    "            max_epochs=Config.NUM_EPOCHS,\n",
    "            logger=tb_logger,\n",
    "            auto_lr_find=True,    \n",
    "            precision=Config.PRECISION,    \n",
    "            weights_summary=None,                    \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        if find_lr:\n",
    "            trainer.tune(model=signs_model, train_dataloaders=dl_train)\n",
    "            print(signs_model.lr)\n",
    "        trainer.fit(signs_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        fold_loss.append(loss_chkpt_callback.best_model_score)\n",
    "        fold_acc.append(acc_chkpt_callback.best_metric)\n",
    "        print(f\"Loss for {fold_str} = {fold_loss[fold]}, accuracy = {fold_acc[fold]}\")\n",
    "        del trainer, signs_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (14) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dedc646af0476ab61dbe3792d163c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0010964781961431851\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010964781961431851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1786b4d26c354aa8abbb763ac0cea9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.1328125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512334fddaf74e4280f719bb9fdb9f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9a787b6deb4082a35761ed8d1c6075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.293\n",
      "Epoch 0, global step 13: val_loss reached 1.29323 (best 1.29323), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=0_val_loss=1.2932.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.46296295523643494\n",
      "metric val_acc = 0.46296295523643494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034b8f0692be46a9881bc1bf21dbe777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.344 >= min_delta = 0.0. New best score: 0.950\n",
      "Epoch 1, global step 27: val_loss reached 0.94973 (best 0.94973), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=1_val_loss=0.9497.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.7870370149612427\n",
      "metric val_acc = 0.7870370149612427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/finetuning.py:215: UserWarning: The provided params to be freezed already exist within another group of this optimizer. Those parameters will be skipped.\n",
      "HINT: Did you init your optimizer in `configure_optimizer` as such:\n",
      " <class 'torch.optim.adam.Adam'>(filter(lambda p: p.requires_grad, self.parameters()), ...) \n",
      "  rank_zero_warn(\n",
      "Current lr: 0.001096478196, Backbone lr: 0.00010964782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4404137aacf34ea584b7cd4a458307b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.772 >= min_delta = 0.0. New best score: 0.177\n",
      "Epoch 2, global step 41: val_loss reached 0.17749 (best 0.17749), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=2_val_loss=0.1775.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000164471729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5fdac42413457990c0c9a5ffac2979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.079 >= min_delta = 0.0. New best score: 0.098\n",
      "Epoch 3, global step 55: val_loss reached 0.09819 (best 0.09819), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=3_val_loss=0.0982.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000246707594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c42daadb7f42bb90c01cfe602ba05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 69: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000370061391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9305555820465088\n",
      "metric val_acc = 0.9305555820465088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3243b7282d54ccd8f2fe7c9c9db546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000555092087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8888888955116272\n",
      "metric val_acc = 0.8888888955116272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f4bddaf8324025ba679fd1b8a195b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.00083263813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9351851940155029\n",
      "metric val_acc = 0.9351851940155029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d863d4eb0ab471f985d8ced5f826cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.001096478196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8472222089767456\n",
      "metric val_acc = 0.8472222089767456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d0b715f8464294b630f4d1cee777f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.0. New best score: 0.085\n",
      "Epoch 8, global step 125: val_loss reached 0.08531 (best 0.08531), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.0853.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.001096478196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af72418274b94f15a6f9f54cefe358c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 139: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n",
      "metric val_acc = 0.9537037014961243\n",
      "Loss for fold0 = 0.08530975133180618, accuracy = 0.9768518805503845\n",
      "Running training for fold1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802a86f3ca084aa381ecae32eb5ba5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0010964781961431851\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010964781961431851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648b91f631d6452cbdc3fcad67acbe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.09375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116b3b5db31e4e94b44251cdb75b90fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d29c01ad4043dfa46b2d5873d2f6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.294\n",
      "Epoch 0, global step 13: val_loss reached 1.29439 (best 1.29439), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold1_best_model_epoch=0_val_loss=1.2944.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.46759259700775146\n",
      "metric val_acc = 0.46759259700775146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80fcac257df43c98778ad1871e63489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.395 >= min_delta = 0.0. New best score: 0.900\n",
      "Epoch 1, global step 27: val_loss reached 0.89976 (best 0.89976), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold1_best_model_epoch=1_val_loss=0.8998.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.7777777910232544\n",
      "metric val_acc = 0.7777777910232544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.00010964782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff9bf4fcd7a41f8869736907483cc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.743 >= min_delta = 0.0. New best score: 0.156\n",
      "Epoch 2, global step 41: val_loss reached 0.15639 (best 0.15639), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold1_best_model_epoch=2_val_loss=0.1564.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000164471729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64aaec3938c04462ba0749bd51e1334e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.090 >= min_delta = 0.0. New best score: 0.066\n",
      "Epoch 3, global step 55: val_loss reached 0.06604 (best 0.06604), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold1_best_model_epoch=3_val_loss=0.0660.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.001096478196, Backbone lr: 0.000246707594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7413f22cb5c14dda8c84f4587b2d5fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 69: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000370061391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9444444179534912\n",
      "metric val_acc = 0.9444444179534912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf3cdb138db494bb270e89fb423f9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.000555092087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8888888955116272\n",
      "metric val_acc = 0.8888888955116272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa2d3e9247b4e0cba2cdfcd76d0fffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.00083263813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9259259104728699\n",
      "metric val_acc = 0.9259259104728699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c5ef21531b4ae393acb4d4687ca11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.001096478196, Backbone lr: 0.001096478196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8611111044883728\n",
      "metric val_acc = 0.8611111044883728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd288b1c41754604a56f772ccfff9c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.066. Signaling Trainer to stop.\n",
      "Epoch 8, global step 125: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8564814925193787\n",
      "metric val_acc = 0.8564814925193787\n",
      "Loss for fold1 = 0.06604068726301193, accuracy = 0.9768518805503845\n",
      "Running training for fold2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f08118af0d43e589a99b48b085447d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0007585775750291836\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007585775750291836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda830a229c14d758132da2fa651ea90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.109375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e4a8af85d64cefbef4b0eed97986fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8112b6094d40c997c9e51fc919be35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.414\n",
      "Epoch 0, global step 13: val_loss reached 1.41392 (best 1.41392), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold2_best_model_epoch=0_val_loss=1.4139.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.49537035822868347\n",
      "metric val_acc = 0.49537035822868347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f61ddbb87e4743852cd77738919213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.222 >= min_delta = 0.0. New best score: 1.192\n",
      "Epoch 1, global step 27: val_loss reached 1.19215 (best 1.19215), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold2_best_model_epoch=1_val_loss=1.1922.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.75\n",
      "metric val_acc = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000758577575, Backbone lr: 7.5857758e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e6533dc2e2484d84db6b24a0bc1891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.764 >= min_delta = 0.0. New best score: 0.428\n",
      "Epoch 2, global step 41: val_loss reached 0.42814 (best 0.42814), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold2_best_model_epoch=2_val_loss=0.4281.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8796296119689941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000758577575, Backbone lr: 0.000113786636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8796296119689941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b374294cfa44150810952654170701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.322 >= min_delta = 0.0. New best score: 0.106\n",
      "Epoch 3, global step 55: val_loss reached 0.10625 (best 0.10625), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold2_best_model_epoch=3_val_loss=0.1063.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000758577575, Backbone lr: 0.000170679954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9537037014961243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246675f6100450a89af2063d225f063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 69: val_loss was not in top 1\n",
      "Current lr: 0.000758577575, Backbone lr: 0.000256019932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9629629850387573\n",
      "metric val_acc = 0.9629629850387573\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72140eb0d0614b7f9e3690334723daee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.000758577575, Backbone lr: 0.000384029897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9675925970077515\n",
      "metric val_acc = 0.9675925970077515\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2b1aa05713487b8cc123ea5f063459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.000758577575, Backbone lr: 0.000576044846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8611111044883728\n",
      "metric val_acc = 0.8611111044883728\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fd59130f364564b581c62e692d749b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.000758577575, Backbone lr: 0.000758577575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9027777910232544\n",
      "metric val_acc = 0.9027777910232544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c79fec210f34336b3a3d9156ceeb19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.106. Signaling Trainer to stop.\n",
      "Epoch 8, global step 125: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9120370149612427\n",
      "metric val_acc = 0.9120370149612427\n",
      "Loss for fold2 = 0.10625462979078293, accuracy = 0.9675925970077515\n",
      "Running training for fold3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f58540557dc4395aa49d3e622f5fdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0009120108393559097\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009120108393559097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a4a93aa4c848a5acbc238a96bb1668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.203125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f744cc519b0c4909b20d884e25448cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd978bf68784b6e91b6af451169688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.359\n",
      "Epoch 0, global step 13: val_loss reached 1.35909 (best 1.35909), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold3_best_model_epoch=0_val_loss=1.3591.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.47685185074806213\n",
      "metric val_acc = 0.47685185074806213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ecd71f7f1f4293bee7372601420df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.314 >= min_delta = 0.0. New best score: 1.045\n",
      "Epoch 1, global step 27: val_loss reached 1.04490 (best 1.04490), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold3_best_model_epoch=1_val_loss=1.0449.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.7268518805503845\n",
      "metric val_acc = 0.7268518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 9.1201084e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3040980eddba44b7ac91ffa6dc374477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.781 >= min_delta = 0.0. New best score: 0.264\n",
      "Epoch 2, global step 41: val_loss reached 0.26352 (best 0.26352), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold3_best_model_epoch=2_val_loss=0.2635.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9444444179534912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 0.000136801626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9444444179534912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b78fd0fbd448b58debd656cecbd7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.198 >= min_delta = 0.0. New best score: 0.066\n",
      "Epoch 3, global step 55: val_loss reached 0.06578 (best 0.06578), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold3_best_model_epoch=3_val_loss=0.0658.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 0.000205202439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b466125beb3d4aa1ad363846d4e654b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 69: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000307803658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9722222089767456\n",
      "metric val_acc = 0.9722222089767456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6949f278563b4f7ca7b6fef42b37f24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000461705487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n",
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4ffd0caf3343679c07efeee7058714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000692558231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9583333134651184\n",
      "metric val_acc = 0.9583333134651184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b147a7cb848c42eba9af7493e6aee2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000912010839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9212962985038757\n",
      "metric val_acc = 0.9212962985038757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810e2280cabd4a79a141277c3bfc136d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.066. Signaling Trainer to stop.\n",
      "Epoch 8, global step 125: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9212962985038757\n",
      "metric val_acc = 0.9212962985038757\n",
      "Loss for fold3 = 0.06578157842159271, accuracy = 0.9768518805503845\n",
      "Running training for fold4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d86d5fe99334abab9594b407ea8480e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.0009120108393559097\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009120108393559097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89af2d36ab3249f9909f4c23f850a149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.109375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c77cb887374ccd8fa941bf0f9b49cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0490ca5dab4667ad579e1369438179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.310\n",
      "Epoch 0, global step 13: val_loss reached 1.30966 (best 1.30966), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold4_best_model_epoch=0_val_loss=1.3097.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.46759259700775146\n",
      "metric val_acc = 0.46759259700775146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928c31969e64b81af90cd19aa1977eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.287 >= min_delta = 0.0. New best score: 1.022\n",
      "Epoch 1, global step 27: val_loss reached 1.02233 (best 1.02233), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold4_best_model_epoch=1_val_loss=1.0223.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8194444179534912\n",
      "metric val_acc = 0.8194444179534912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 9.1201084e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4fad94a9e042eca338e2121ddcea40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.645 >= min_delta = 0.0. New best score: 0.377\n",
      "Epoch 2, global step 41: val_loss reached 0.37717 (best 0.37717), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold4_best_model_epoch=2_val_loss=0.3772.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8564814925193787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 0.000136801626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8564814925193787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7c5b98ee7a4a3fbeb351d6127765cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.304 >= min_delta = 0.0. New best score: 0.073\n",
      "Epoch 3, global step 55: val_loss reached 0.07305 (best 0.07305), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold4_best_model_epoch=3_val_loss=0.0730.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 0.000205202439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def90ea6afcb4832ac6aa0e23902dd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.070\n",
      "Epoch 4, global step 69: val_loss reached 0.06996 (best 0.06996), saving model to \"/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold4_best_model_epoch=4_val_loss=0.0700.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9722222089767456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current lr: 0.000912010839, Backbone lr: 0.000307803658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9722222089767456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73c7d92867840bf87c092c889617c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 83: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000461705487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9398148059844971\n",
      "metric val_acc = 0.9398148059844971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb544472cab74f28b6dcc3c1a5f9f655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 97: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000692558231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9768518805503845\n",
      "metric val_acc = 0.9768518805503845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b1448051d34bf5a3517b00df0eabff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 111: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000912010839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.9120370149612427\n",
      "metric val_acc = 0.9120370149612427\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4349811a82545499c3ea8fc4d07e1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 125: val_loss was not in top 1\n",
      "Current lr: 0.000912010839, Backbone lr: 0.000912010839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8472222089767456\n",
      "metric val_acc = 0.8472222089767456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c69425206b4a4c86c607b678e40dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.070. Signaling Trainer to stop.\n",
      "Epoch 9, global step 139: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_acc = 0.8796296119689941\n",
      "metric val_acc = 0.8796296119689941\n",
      "Loss for fold4 = 0.06996455043554306, accuracy = 0.9768518805503845\n"
     ]
    }
   ],
   "source": [
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "\n",
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, k_folds, train_x, train_y)\n",
    "    run_training(fold, dl_train, dl_val)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss across folds\n",
      "[tensor(0.0853, device='cuda:0'), tensor(0.0660, device='cuda:0'), tensor(0.1063, device='cuda:0'), tensor(0.0658, device='cuda:0'), tensor(0.0700, device='cuda:0')]\n",
      "Accuracy across folds\n",
      "[0.9768518805503845, 0.9768518805503845, 0.9675925970077515, 0.9768518805503845, 0.9768518805503845]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss across folds\")\n",
    "print(fold_loss)\n",
    "print(\"Accuracy across folds\")\n",
    "print(fold_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7758/2127697761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = ImageClassificationLitModel.load_from_checkpoint(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0mcompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bk_anupam/code/ML/DeepLearningWSL/ComputerVision/CNN/model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "model = ImageClassificationLitModel.load_from_checkpoint(\n",
    "    checkpoint_path=\"model/fold0_best_model_epoch=8_val_loss=0.0853-v1.ckpt\",     \n",
    "    num_classes=Config.NUM_CLASSES\n",
    "    )\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "incorrect = 0\n",
    "total = 0\n",
    "predicted_labels_incorrect = []\n",
    "labels_incorrect = []\n",
    "with torch.no_grad():\n",
    "    counter=0\n",
    "    for imgs, labels in tqdm.tqdm(dl_val):                \n",
    "        predicted_cuda_labels = torch.argmax(model(imgs.to(\"cuda\")), dim=1)\n",
    "        predicted_labels = predicted_cuda_labels.cpu().detach()\n",
    "        total += labels.shape[0]\n",
    "        correct_pred = predicted_labels == labels\n",
    "        incorrect_pred = ~correct_pred\n",
    "        num_incorrect_pred = incorrect_pred.sum()\n",
    "        incorrect += int(num_incorrect_pred)\n",
    "        if num_incorrect_pred > 0:\n",
    "            predicted_labels_incorrect.append(predicted_labels[incorrect_pred].numpy())\n",
    "            labels_incorrect.append(labels[incorrect_pred].numpy())\n",
    "print(f'Total no. of images in validation set: {total}')\n",
    "print(f'Incorrectly classified images in validation set: {incorrect}')\n",
    "accuracy = ((total-incorrect) / total) * 100        \n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "148f3469c78c75f496aee59433c1c8be3c885ccea1b507530cbeda0a24e0e40d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('fastai': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
