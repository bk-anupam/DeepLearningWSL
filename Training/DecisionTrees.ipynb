{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entropy**\n",
    "The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable.\n",
    "\n",
    "Given a probability distribution p over a random variable X =>  $H(p) = E_p[-\\log_2p]$ <br>\n",
    "<br>\n",
    "**Discrete case =>** $H(p) =\\ sum_{x \\in X} p(x)log_2p(x)$ <br>\n",
    "<br>\n",
    "**Continuous case =>** $H(p) = \\int_X p(x)log_2p(x)dx$\n",
    "\n",
    "If all events are equally likely, there is more surprise hence entropy is more (more bits are required to represent the event). On the contrary if some events are more likely than others, entropy will be less for such a probability distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mutual information and information gain**\n",
    "Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees.Information gain is calculated by comparing the entropy of the dataset before and after a transformation. The transformation is splitting the dataset on a feature (categorical or continuous).\n",
    "\n",
    "Consider a dataset like this\n",
    "\n",
    "<img src=\"images/decision_trees/pic1.jpg\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "We have multiple categorical variables and one target variable. Let's say we split the dataset on feature \"Windy\". Calculate the entropy of the dataset before and after the split. The difference is the information gain. The feature that leads to maximum information gain is the best feature to split. <br>\n",
    "In case of continuous variable, first sort the data in ascending order, then take the average of two consecutive values and use them as the splitting threshold. The threshold that leads to maximum information gain is the splitting threshold for the continuous features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index\n",
    "\n",
    "Gini index => $1-\\sum\\limits_{c=1}^{C}(p_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression Trees**\n",
    "If the target variable is continuous, the decision tree is a regression tree.\n",
    "In a regression tree how do we determine the threshold to be used for splitting the data on continuous feature variable? First sort the data in ascending order, then take the average of two consecutive values and use them as the splitting threshold. Calculate the mean squared error (square of the difference between the observed and the predicted value for each data point and take its average). For each splitting threshold, the one that gives the least MSE is the threshold to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
